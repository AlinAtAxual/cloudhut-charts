# Default values for kafka-owl.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

nameOverride: ""
fullnameOverride: ""

imagePullSecrets: []
image:
  repository: quay.io/kafka-owl/kafka-owl
  tag: latest
  pullPolicy: IfNotPresent

# Kafka Owl Settings
server:
  http:
    listenPort: 8080

logging:
  level: "info" # info, debug, warn, error, panic, fatal

kafka:
  # brokers:
  version: "0.11.0.2"
  clientId: "kafka-owl"
  sasl:
    enabled: false
    useHandshake: true
    # existingSecretName: secretname
    # username:
    # password:
  tls:
    enabled: false
    # You can either create the secret yourself or let the helm chart create one for you.
    # If you create the secret yourself, specify the secret name under existingSecretName.
    # Make sure the key names of your secret comply with those used in this chart.
    #
    # existingSecretName: secretname
    # ca: |-
    #   -----BEGIN CERTIFICATE-----
    #   ...
    #   -----END CERTIFICATE-----
    # cert:
    # key:
    passphrase: ""
    insecureSkipVerify: false


service:
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

podSecurityContext:
  runAsUser: 99
  fsGroup: 99

containerSecurityContext: {}

# Create ServiceMonitor custom resource required for Prometheus Operator to scrape the service
serviceMonitor:
  create: false
  interval: 10s
  scrapeTimeout: 10s
  # By default, Prometheus Operator uses its own Release label as the selector for ServiceMonitors.
  # Update this to match the release name of your Prometheus Operator installation if 
  # you aren't using custom match labels on your Prometheus definition.
  releaseLabel: prometheus-operator
  # Additional labels to add to the ServiceMonitor in case Prometheus Operator is configured with 
  # different matchLabels configuration, to make sure it matches this ServiceMonitor.
  # If this is defined, the release label of the service monitor will match kube-eagle's one
  # additionalLabels:
